{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Analyzing IMDB Data in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Loading the data\n",
    "This dataset comes preloaded with Keras, so one simple command will get us training and testing data. There is a parameter for how many words we want to look at. We've set it at 1000, but feel free to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data (it's preloaded in Keras)\n",
    "num_features = 5000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_features)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Examining the data\n",
    "Notice that the data has been already pre-processed, where all the words have numbers, and the reviews come in as a vector with the words that the review contains. For example, if the word 'the' is the first one in our dictionary, and a review contains the word 'the', then there is a 1 in the corresponding vector.\n",
    "\n",
    "The output comes as a vector of 1's and 0's, where 1 is a positive sentiment for the review, and 0 is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. One-hot encoding the output\n",
    "Here, we'll turn the input vectors into (0,1)-vectors. For example, if the pre-processed vector contains the number 14, then in the processed vector, the 14th entry will be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# One-hot encoding the output into vector mode, each of length 1000\n",
    "tokenizer = Tokenizer(num_words=num_features)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "# print(x_train[0]) # convert each array to one-hot 1000 colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n",
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And we'll also one-hot encode the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding the output\n",
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Building the  model architecture\n",
    "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_49 (Dense)             (None, 128)               640128    \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 648,514.0\n",
      "Trainable params: 648,514.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TODO: Build the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='sigmoid', input_shape=(num_features,))) # 25000 data points, each has num_features\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(2, activation='softmax')) # 2 output\n",
    "\n",
    "# TODO: Compile the model using a loss function and an optimizer.\n",
    "model.compile(loss = 'mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5. Training the model\n",
    "Run the model here. Experiment with different batch_size, and number of epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time:  65.99659895896912\n"
     ]
    }
   ],
   "source": [
    "# TODO: Run the model. Feel free to experiment with different batch sizes and number of epochs.\n",
    "t0 = time.time()\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=500, verbose=0) # total 25000 data\n",
    "print (\"Run time: \", time.time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 6. Evaluating the model\n",
    "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8662\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Result from different parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change epochs\n",
    "- epochs = 5 ,batch_size=500, num_features = 1000, activation='relu', loss = 'categorical_crossentropy', optimizer='adam'\n",
    "    - Accuracy:  0.846\n",
    "    - Run time:  3.30\n",
    "- epochs = 10, batch_size=500, num_features = 1000, activation='relu', loss = 'categorical_crossentropy', optimizer='adam'\n",
    "    - Accuracy:  0.84696\n",
    "    - Run time:  7.26\n",
    "- epochs = 20 ,batch_size=500, num_features = 1000, activation='relu', loss = 'categorical_crossentropy', optimizer='adam'\n",
    "    - Accuracy:  0.84772\n",
    "    - Run time:  14.27\n",
    "- epochs = 50, batch_size=500, num_features = 1000, activation='relu', loss = 'categorical_crossentropy', optimizer='adam'\n",
    "    - Accuracy: 0.84632\n",
    "    - Run time:  35.62\n",
    "- epochs = 100,batch_size=500, num_features = 1000, activation='relu', loss = 'categorical_crossentropy', optimizer='adam'\n",
    "    - Accuracy:  0.84296\n",
    "    - Run time:  68.66\n",
    "- Increasing epochs to 20 tend to increase accuracy with better fitting. Further increasing epochs decreases accuracy on test data due to overfitting. It also takes longer time to iterate for more epochs (linear to epochs). \n",
    "### Change batch size\n",
    "- epochs = 20, batch_size=100, num_features = 1000, activation='relu', loss = 'categorical_crossentropy', optimizer='adam'\n",
    "    - Accuracy:  0.84648\n",
    "    - Run time:  21.89\n",
    "- epochs = 20, batch_size=250, num_features = 1000, activation='relu', loss = 'categorical_crossentropy', optimizer='adam'\n",
    "    - Accuracy: 0.84752\n",
    "    - Run time:  15.39\n",
    "- epochs = 20, batch_size=500, num_features = 1000, activation='relu', loss = 'categorical_crossentropy', optimizer='adam'\n",
    "    - Accuracy:  0.84772\n",
    "    - Run time:  14.27\n",
    "- epochs = 20, batch_size=1000, num_features = 1000, activation='relu', loss = 'categorical_crossentropy', optimizer='adam'\n",
    "    - Accuracy:  0.84724\n",
    "    - Run time:  12.29\n",
    "- epochs = 20, batch_size=5000,num_features = 1000, activation='relu', loss = 'categorical_crossentropy', optimizer='adam'\n",
    "    - Accuracy:  0.84764\n",
    "    - Run time:  12.37\n",
    "- Too small batch_size has low accuracy. Increasing batch_size seems to increase accuracy with shorter run time. \n",
    "### Change num_features\n",
    "- epochs = 20, batch_size=500, num_features = 500, activation='relu', loss = 'categorical_crossentropy', optimizer='adam'\n",
    "    - Accuracy:  0.8116\n",
    "    - Run time:  10.42\n",
    "- epochs = 20, batch_size=500, num_features = 1000, activation='relu', loss = 'categorical_crossentropy', optimizer='adam'\n",
    "    - Accuracy:  0.84772\n",
    "    - Run time:  14.27\n",
    "- epochs = 20, batch_size=500, num_features = 2000, activation='relu', loss = 'categorical_crossentropy', optimizer='adam'\n",
    "    - Accuracy:  0.85708\n",
    "    - Run time:  26.22\n",
    "- epochs = 20, batch_size=500, num_features = 5000, activation='relu', loss = 'categorical_crossentropy', optimizer='adam'\n",
    "    - Accuracy:  0.86348\n",
    "    - Run time:  66.58\n",
    "- More features and number of words tend to increase accuracy but takes longer time to train the model. \n",
    "### Change activation\n",
    "- epochs = 20, batch_size=500, num_features = 1000, activation='relu', loss = 'categorical_crossentropy', optimizer='adam'\n",
    "    - Accuracy:  0.84772\n",
    "    - Run time:  14.27\n",
    "- epochs = 20, batch_size=500, num_features = 1000, activation='sigmoid', loss = 'categorical_crossentropy', optimizer='adam'\n",
    "    - Accuracy:  0.86056\n",
    "    - Run time:  16.39\n",
    "- It seems sigmoid activation works better\n",
    "### Change loss\n",
    "- epochs = 20, batch_size=500, num_features = 1000, activation='relu', loss = 'categorical_crossentropy', optimizer='adam'\n",
    "    - Accuracy:  0.84772\n",
    "    - Run time:  14.27\n",
    "- epochs = 20, batch_size=500, num_features = 1000, activation='relu', loss = 'mean_squared_error', optimizer='adam'\n",
    "    - Accuracy:  0.84824\n",
    "    - Run time:  15.88\n",
    "- It seems mean_squared_error works better\n",
    "### Change optimizer\n",
    "- epochs = 20, batch_size=500, num_features = 1000, activation='relu', loss = 'categorical_crossentropy', optimizer='adam'\n",
    "    - Accuracy:  0.84772\n",
    "    - Run time:  14.27\n",
    "- epochs = 20, batch_size=500, num_features = 1000, activation='relu', loss = 'categorical_crossentropy', optimizer='rmsprop'\n",
    "    - Accuracy:  0.8394\n",
    "    - Run time:  14.62\n",
    "- epochs = 20, batch_size=500, num_features = 1000, activation='relu', loss = 'categorical_crossentropy', optimizer='sgd'\n",
    "    - Accuracy:  0.80852\n",
    "    - Run time:  14.45\n",
    "- adam optimizer works better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Combine parameters\n",
    "- epochs = 20, batch_size=500, num_features = 1000, activation='relu', loss = 'categorical_crossentropy', optimizer='adam'\n",
    "    - Accuracy:  0.84772\n",
    "    - Run time:  14.27\n",
    "- epochs = 20, batch_size=500, num_features = 5000, activation='sigmoid', loss = 'mean_squared_error', optimizer='adam'\n",
    "    - Accuracy:  0.8662\n",
    "    - Run time:  65.99\n",
    "- Achieve a high accuracy of 86.62%. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
